# -*- coding: utf-8 -*-
"""Redes neuronales con GPT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L81A5nIxUpt16KC_HtZY9hPx7gZakH0_

## **Aprendiendo redes neuronales con ChatGPT**
*Dorys Trujillo Beltrán*

*04 noviembre 2023*

**1. En la clase de Machine Learning nos dan el siguente código que pertenece a un perceptrón.**
"""

# Unidad mas pequeña de una red neuronal
def perceptron(x1,x2):
    v = x1 + x2 - 1.2
    if v < 0:
        return 0
    else:
        return 1

X = [[0, 0],
    [0, 1],
    [1, 0],
    [1, 1]]

for n in range(4):
    y = perceptron(X[n][0],X[n][1])
    print('Input :', X[n], 'Output :', y)

"""**Construcción del perceptrón con paquete de Python**

Interpretación del output generada por GPT:
* Para la primera entrada [0, 0], el perceptrón produce una salida de 0.
* Para la segunda entrada [0, 1], el perceptrón también produce una salida de 0.
* Para la tercera entrada [1, 0], el perceptrón nuevamente produce una salida de 0.
* Para la cuarta entrada [1, 1], el perceptrón produce una salida de 1.
"""

import numpy as np

def perceptron2(Xmat):
    W = np.array([1, 1])
    V = Xmat @ W.T - 1.2
    Y = list(map(lambda x: 0 if x<0 else 1, V))
    return Y

print('Output', perceptron2(X))

"""**2. Ahora tengo el siguiente ejemplo**"""

def neuron(Xmat, W, b):
    V = Xmat @ W.T + b
    return list(map(lambda x: 0 if x<0 else 1, V))

def MLP_XOR(Xmat):
    W1 = np.array([1, 1])
    b1 = -1.5
    Y1 = neuron(Xmat, W1, b1)
    W2 = np.array([1, 1])
    b2 = -0.5
    Y2 = neuron(Xmat, W2, b2)
    X3 = np.c_[Y1, Y2]
    W3 = np.array([-2, 1])
    b3 = -0.5
    return neuron(X3, W3, b3)

MLP_XOR(X)

"""**3. Ahora me plantean el siguiente ejercicio:**

Modifique la función anterior de modo que utilice ReLU y funciones de activación sigmoidea de las neuronas en la capa oculta y la capa de salida, respectivamente. Verifique si la nueva arquitectura todavía implementa un XOR.
"""

# Código generado por GPT modificado para que cumpla con lo que el ejercicio pide
import math   #Libreria

def sigmoid(v):  #definiendo función de activación sigmoidea
    return 1 / (1+math.exp(-v))


def relu(v):  #definiendo función de activación relu
    return np.max(v, 0)


def neuron(Xmat, W, b, activation = 'relu'): # función de la neurona con pesos y determinando relu con func de activación
    V = Xmat @ W.T + b
    if activation == 'sigmoid':     #se introduce un condicional para determinar las salidas dependiendo la func de activación
      Y = [sigmoid(v) for v in V]
    else:
      Y = [relu(v) for v in V]
    return Y

def MLP_XOR(Xmat):
    W1 = np.array([1, 1])
    b1 = -1.5
    Y1 = neuron(Xmat, W1, b1)
    W2 = np.array([1,1])   #definiendo el peso 2 que a GPT le faltó
    b2 = -0.5
    Y2 = neuron(Xmat, W2, b2)

    X3= np.c_[Y1, Y2]
    W3 = np.array([-2,1])
    b3= -0.5

    return neuron(X3, W3, b3, activation = 'sigmoid')

MLP_XOR(X)

"""**Nota:**

ChatGPT me ayudó a entender las líneas de código de los ejemplos que el profesor de Machine Learning nos da en clase, que solo con la clase no lo logré entender y además a fortalecer mi comprensión con código en python.
"""